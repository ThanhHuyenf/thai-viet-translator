from model.dataset import load_dictionary, build_graph
from model.gcn_model import GCN
import torch
import torch.nn.functional as F
import torch.optim as optim


def train_gcn(data, num_epochs=200):
    model = GCN(
        input_dim=data.x.size(1),
        hidden_dim=32,
        output_dim=data.x.size(1)
    )
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        # Má»¥c tiÃªu tá»± giÃ¡m sÃ¡t: tÃ¡i táº¡o láº¡i Ä‘áº·c trÆ°ng ban Ä‘áº§u
        loss = F.mse_loss(out, data.x)
        loss.backward()
        optimizer.step()
        if epoch % 50 == 0:
            print(f"Epoch {epoch:03d} | Loss: {loss.item():.6f}")
    return model


def get_embedding(word, model, data, df):
    """
    Tráº£ vá» embedding cho tá»« cá»¥ thá»ƒ trong cá»™t 'TuNgu'.
    """
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)

    # TÃ¬m index cá»§a tá»« trong DataFrame
    indices = df.index[df['TuNgu'] == word].tolist()
    if not indices:
        raise ValueError(f"Tá»« '{word}' khÃ´ng cÃ³ trong dá»¯ liá»‡u.")
    idx = indices[0]

    return out[idx]


if __name__ == "__main__":
    print("ğŸ”¹ Äang táº£i dá»¯ liá»‡u tá»« Ä‘iá»ƒn...")
    df = load_dictionary("data/dic.csv")

    print("ğŸ”¹ Äang xÃ¢y dá»±ng Ä‘á»“ thá»‹...")
    data = build_graph(df)

    print("ğŸ”¹ Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh GCN...")
    model = train_gcn(data, num_epochs=200)

    word = "êª€êª±"
    emb = get_embedding(word, model, data, df)

    print(f"\nâœ… Embedding cho '{word}':")
    print(emb)
